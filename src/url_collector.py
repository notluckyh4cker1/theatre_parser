import re
from datetime import datetime, timedelta
import requests
import time
import os
import random
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import config

class URLCollector:
    def __init__(self):
        self.base_url = config.BASE_URL
        self.theater_url = config.THEATER_URL
        self.headers = config.HEADERS
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.play_urls = set()

        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',
        ]

    def get_soup(self, url):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Ä–æ—Ç–∞—Ü–∏–µ–π User-Agent –∏ —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏"""
        try:
            headers = self.headers.copy()
            headers['User-Agent'] = random.choice(self.user_agents)

            time.sleep(random.uniform(2, 5))

            response = self.session.get(url, headers=headers, timeout=config.TIMEOUT)
            response.raise_for_status()

            if 'captcha' in response.text.lower() or '–¥–æ—Å—Ç—É–ø –≤—Ä–µ–º–µ–Ω–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω' in response.text:
                print(f"‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–∞—è –∫–∞–ø—á–∞ –Ω–∞ {url}")
                time.sleep(10)

            return BeautifulSoup(response.text, 'html.parser')
        except requests.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")

            if '429' in str(e):
                print(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤. –ñ–¥–µ–º 30 —Å–µ–∫—É–Ω–¥...")
                time.sleep(30)

            return None

    def extract_play_urls_from_page(self, soup):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–ø–µ–∫—Ç–∞–∫–ª–∏ - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        urls = []

        # 1. –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤ –∫–∞–ª–µ–Ω–¥–∞—Ä–µ/—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–∏
        calendar_links = soup.find_all('a', href=re.compile(r'/teatr/.*/\d{4}-\d{2}-\d{2}'))
        for link in calendar_links:
            href = link['href']
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π URL —Å–ø–µ–∫—Ç–∞–∫–ª—è (–±–µ–∑ –¥–∞—Ç—ã)
            match = re.match(r'(/teatr/[^/]+)/\d{4}-\d{2}-\d{2}', href)
            if match:
                base_url = match.group(1)
                full_url = urljoin(self.base_url, base_url)
                if full_url not in urls:
                    urls.append(full_url)

        # 2. –ò—â–µ–º –æ–±—ã—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–ø–µ–∫—Ç–∞–∫–ª–∏
        play_patterns = [
            r'/teatr/[^/\s]+',
            r'/event/[^/\s]+',
        ]

        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link['href']

            for pattern in play_patterns:
                if re.match(pattern, href):
                    full_url = urljoin(self.base_url, href)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–ø–µ–∫—Ç–∞–∫–ª—å
                    if (full_url.startswith(f"{self.base_url}/teatr/") and
                            len(full_url) > len(self.base_url) + 10 and
                            'category' not in full_url.lower() and
                            'tag' not in full_url.lower() and
                            'author' not in full_url.lower()):

                        # –£–±–∏—Ä–∞–µ–º –¥–∞—Ç—É –∏–∑ –∫–æ–Ω—Ü–∞ URL –µ—Å–ª–∏ –µ—Å—Ç—å
                        full_url = re.sub(r'/\d{4}-\d{2}-\d{2}$', '', full_url)

                        if full_url not in urls:
                            urls.append(full_url)

        return list(set(urls))

    def collect_urls_from_categories(self):
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        categories = [
            '',
            'myuzikl',
            'drama',
            'komediya',
            'balet',
            'opera',
            'detektiv',
            'skazki',
            'melodrama',
            'klassicheskaya-drama',
            'sovremennaya-drama',
            'veselye-komedii',
        ]

        all_urls = set()

        for category in categories:
            try:
                if category:
                    category_url = f"{self.theater_url}/{category}"
                    print(f"üìÇ –°–æ–±–∏—Ä–∞–µ–º –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
                else:
                    category_url = self.theater_url
                    print(f"üìÇ –°–æ–±–∏—Ä–∞–µ–º —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã")

                for page in range(1, 6):
                    if page > 1:
                        if category:
                            page_url = f"{category_url}?page={page}"
                        else:
                            page_url = f"{category_url}?page={page}"
                    else:
                        page_url = category_url

                    print(f"   –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}...")
                    soup = self.get_soup(page_url)

                    if not soup:
                        break

                    page_urls = self.extract_play_urls_from_page(soup)
                    all_urls.update(page_urls)

                    print(f"     –ù–∞–π–¥–µ–Ω–æ {len(page_urls)} —Å—Å—ã–ª–æ–∫ (–≤—Å–µ–≥–æ: {len(all_urls)})")

                    if not self.has_next_page(soup):
                        break

                    time.sleep(random.uniform(3, 7))

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞: {e}")
                continue

        return list(all_urls)

    def has_next_page(self, soup):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
        next_button = soup.find('a', text=re.compile(r'–¥–∞–ª—å—à–µ|—Å–ª–µ–¥—É—é—â–∞—è|next', re.I))
        return next_button is not None

    def collect_urls_from_calendar(self):
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–µ 30 –¥–Ω–µ–π"""
        print("üìÖ –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—é (30 –¥–Ω–µ–π)...")

        all_urls = set()
        today = datetime.now()

        for i in range(30):  # –°–ª–µ–¥—É—é—â–∏–µ 30 –¥–Ω–µ–π
            try:
                date = today + timedelta(days=i)
                date_str = date.strftime('%Y-%m-%d')
                date_url = f"{self.theater_url}/{date_str}"

                print(f"   üìÜ {date_str}...")

                soup = self.get_soup(date_url)

                if soup:
                    date_urls = self.extract_play_urls_from_page(soup)
                    all_urls.update(date_urls)
                    print(f"     –ù–∞–π–¥–µ–Ω–æ {len(date_urls)} —Å–ø–µ–∫—Ç–∞–∫–ª–µ–π")

                time.sleep(random.uniform(2, 4))

            except Exception as e:
                print(f"     ‚ùå –û—à–∏–±–∫–∞ –¥–ª—è –¥–∞—Ç—ã {date_str}: {e}")
                continue

        return list(all_urls)

    def collect_all_urls(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫...")

        all_urls = set()

        # –ú–µ—Ç–æ–¥ 1: –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
        print("\nüìã –ú–µ—Ç–æ–¥ 1: –°–±–æ—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        category_urls = self.collect_urls_from_categories()
        all_urls.update(category_urls)
        print(f"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {len(category_urls)} —Å—Å—ã–ª–æ–∫")

        # –ú–µ—Ç–æ–¥ 2: –ö–∞–ª–µ–Ω–¥–∞—Ä—å
        print("\nüìÖ –ú–µ—Ç–æ–¥ 2: –°–±–æ—Ä –ø–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—é")
        calendar_urls = self.collect_urls_from_calendar()
        all_urls.update(calendar_urls)
        print(f"‚úÖ –ö–∞–ª–µ–Ω–¥–∞—Ä—å: {len(calendar_urls)} —Å—Å—ã–ª–æ–∫")

        # –ú–µ—Ç–æ–¥ 3: –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print("\nüî• –ú–µ—Ç–æ–¥ 3: –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ø–µ–∫—Ç–∞–∫–ª–∏")
        popular_urls = self.collect_popular_urls()
        all_urls.update(popular_urls)
        print(f"‚úÖ –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ: {len(popular_urls)} —Å—Å—ã–ª–æ–∫")

        urls = list(all_urls)
        print(f"\nüéâ –ò–¢–û–ì–û —Å–æ–±—Ä–∞–Ω–æ: {len(urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")

        if urls:
            self.save_urls_to_file(urls)

        return urls

    def collect_popular_urls(self):
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ø–µ–∫—Ç–∞–∫–ª–∏"""
        popular_pages = [
            f"{self.theater_url}/popular",
            f"{self.theater_url}/recommendations",
            f"{self.theater_url}/best",
        ]

        urls = set()

        for page_url in popular_pages:
            try:
                soup = self.get_soup(page_url)
                if soup:
                    page_urls = self.extract_play_urls_from_page(soup)
                    urls.update(page_urls)
            except:
                continue

        return list(urls)

    def run(self, force_collect=True):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫"""
        if not force_collect and os.path.exists(config.URLS_FILE):
            urls = self.load_urls_from_file()
            if urls and len(urls) >= 100:
                print(f"üìñ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ ({len(urls)} —à—Ç)")
                return urls

        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫...")
        urls = self.collect_all_urls()

        if len(urls) < 50:
            print(f"‚ö†Ô∏è –°–æ–±—Ä–∞–Ω–æ –º–∞–ª–æ —Å—Å—ã–ª–æ–∫ ({len(urls)}). –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ä—ã–µ...")
            old_urls = self.load_urls_from_file()
            urls = list(set(urls + old_urls))

        return urls[:400]

    def save_urls_to_file(self, urls):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Å—ã–ª–∫–∏ –≤ —Ñ–∞–π–ª"""
        try:
            os.makedirs(os.path.dirname(config.URLS_FILE), exist_ok=True)

            with open(config.URLS_FILE, 'w', encoding='utf-8') as f:
                for url in urls:
                    f.write(url + '\n')
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫ –≤ {config.URLS_FILE}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫: {e}")

    def load_urls_from_file(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(config.URLS_FILE):
                with open(config.URLS_FILE, 'r', encoding='utf-8') as f:
                    urls = [line.strip() for line in f if line.strip()]
                return urls
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Å—ã–ª–æ–∫: {e}")
        return []